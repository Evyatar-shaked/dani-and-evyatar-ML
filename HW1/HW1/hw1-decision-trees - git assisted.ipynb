{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Student #1 ID: 207781956"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student #2 ID: 209193002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6bd0516e7cb654f5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 1: Decision Trees\n",
    "\n",
    "In this assignment you will implement a Decision Tree algorithm as learned in class.\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "1. Write **efficient vectorized** code whenever possible. Some calculations in this exercise take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deduction.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit. Those tests will not be graded nor checked.\n",
    "1. You are free to add code and markdown cells as you see fit.\n",
    "1. Write your functions in this jupyter notebook only. Do not create external python modules and import from them.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only, unless otherwise mentioned.\n",
    "1. Your code must run without errors. It is a good idea to restart the notebook and run it from end to end before you submit your exercise.\n",
    "1. Answers to qualitative questions should be written in **markdown cells (with $\\LaTeX$ support)**.\n",
    "1. Submit this jupyter notebook only using your ID as a filename. **No not use ZIP or RAR**. For example, your submission should look like this: `123456789.ipynb` if you worked by yourself or `123456789_987654321.ipynb` if you worked in pairs.\n",
    "\n",
    "## In this exercise you will perform the following:\n",
    "1. Practice OOP in python.\n",
    "2. Implement two impurity measures: Gini and Entropy.\n",
    "3. Construct a decision tree algorithm.\n",
    "4. Prune the tree to achieve better results.\n",
    "5. Visualize your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed9fe7b1026e33cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Make the notebook automatically reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6ac605270c2b091",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Warmup - OOP in python\n",
    "\n",
    "Our desicion tree will be implemented using a dedicated python class. Take a minute and practice your object oriented skills. Create a tree with some nodes and make sure you understand how objects in python work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, node):\n",
    "        self.children.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Node at 0x191bc5e7610>, <__main__.Node at 0x191bc5e74d0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = Node(5)\n",
    "p = Node(6)\n",
    "q = Node(7)\n",
    "n.add_child(p)\n",
    "n.add_child(q)\n",
    "n.children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2f1ceb251c649b62",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "We will use the breast cancer dataset that is available as a part of sklearn. In this example, our dataset will be a single matrix with the **labels on the last column**. Notice that you are not allowed to use additional functions from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d79cb4542926ad3f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape:  (426, 31)\n",
      "Testing dataset shape:  (143, 31)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load dataset\n",
    "X, y = datasets.load_breast_cancer(return_X_y = True)\n",
    "X = np.column_stack([X,y]) # the last column holds the labels\n",
    "\n",
    "# split dataset\n",
    "X_train, X_test = train_test_split(X, random_state=99)\n",
    "\n",
    "print(\"Training dataset shape: \", X_train.shape)\n",
    "print(\"Testing dataset shape: \", X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fd7b0191f3f1e897",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Impurity Measures (10 points)\n",
    "\n",
    "Impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. Implement the functions `calc_gini` (5 points) and `calc_entropy` (5 points). You are encouraged to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gini(data):\n",
    "    \"\"\"\n",
    "    Calculate gini impurity measure of a dataset.\n",
    " \n",
    "    Input:\n",
    "    - data: any dataset where the last column holds the labels.\n",
    " \n",
    "    Returns the gini impurity.    \n",
    "    \"\"\"\n",
    "   \n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    \n",
    "    # Extract labels (last column)\n",
    "    labels = data[:, -1]\n",
    "    \n",
    "    # If no data, return 0 (pure)\n",
    "    if len(labels) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Count occurrences of each class\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    probabilities = counts / len(labels)\n",
    "    \n",
    "    # Calculate Gini impurity: 1 - sum(p_i^2)\n",
    "    gini = 1.0 - np.sum(probabilities ** 2)\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(data):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a dataset.\n",
    "\n",
    "    Input:\n",
    "    - data: any dataset where the last column holds the labels.\n",
    "\n",
    "    Returns the entropy of the dataset.    \n",
    "    \"\"\"\n",
    "    entropy = 0.0\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    # Extract labels (last column)\n",
    "    labels = data[:, -1]\n",
    "    \n",
    "    # If no data, return 0 (pure)\n",
    "    if len(labels) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "        \n",
    "    # Count occurrences of each class\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    probabilities = counts / len(labels)\n",
    "    \n",
    "    # Calculate Entropy: - sum(p_i * log2(p_i))\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree (50 points)\n",
    "\n",
    "Use a Python class to construct the decision tree. Your class should support the following functionality:\n",
    "\n",
    "1. Initiating a node for a decision tree. You will need to use several class methods and class attributes and you are free to use them as you see fit. We recommend that every node will hold the **feature** and **value** used for the split and the **children** of that node. In addition, it might be a good idea to store the **prediction** in that node, the **height** of the tree for that node and whether or not that node is a **leaf** in the tree.\n",
    "2. Your code should support both Gini and Entropy as impurity measures. \n",
    "3. The provided data includes continuous data. For this exercise, create at most a **single split** for each node of the tree (your tree will be binary). Determine the threshold for splitting by checking all possible features and the values available for splitting. When considering the values, take the average of each consecutive pair. For example, for the values [1,2,3,4,5] you should test possible splits on the values [1.5, 2.5, 3.5, 4.5]. \n",
    "5. After you complete building the class for a decision node in the tree, complete the function `build_tree`. This function takes as input the training dataset and the impurity measure. Then, it initializes a root for the decision tree and constructs the tree according to the procedure you saw in class.\n",
    "1. Once you are finished, construct two trees: one with Gini as an impurity measure and the other using Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    '''\n",
    "    This class will hold everyhing you need to construct a node in a DT. You are required to \n",
    "    support basic functionality as previously described. It is highly recommended that you  \n",
    "    first read and understand the entire exercises before diving into this class.\n",
    "    You are allowed to change the structure of this class as you see fit.\n",
    "    '''\n",
    " \n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Decision Node class\n",
    "# ==============================\n",
    "class DecisionNode:\n",
    "    def __init__(self, data, height=0, max_depth=None, min_samples_split=2):\n",
    "        self.data = data\n",
    "        self.height = height\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.children = []\n",
    "        self.is_leaf = True\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.prediction = None  # majority class if leaf\n",
    "\n",
    "    def add_child(self, node):\n",
    "        self.children.append(node)\n",
    "    \n",
    "    def should_stop_splitting(self):\n",
    "        \"\"\"Check if we should stop splitting this node.\"\"\"\n",
    "        # Stop if max depth reached\n",
    "        if self.max_depth is not None and self.height >= self.max_depth:\n",
    "            return True\n",
    "        \n",
    "        # Stop if not enough samples to split\n",
    "        if len(self.data) < self.min_samples_split:\n",
    "            return True\n",
    "        \n",
    "        # Stop if all samples have the same label (pure node)\n",
    "        if len(np.unique(self.data[:, -1])) == 1:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def split(self, impurity_measure):\n",
    "        \"\"\"\n",
    "        Find the best split for this node using vectorized operations.\n",
    "        More efficient than repeatedly calling check_split.\n",
    "        Respects early stopping criteria.\n",
    "        \"\"\"\n",
    "        # Check if we should stop splitting\n",
    "        if self.should_stop_splitting():\n",
    "            self.is_leaf = True\n",
    "            labels, counts = np.unique(self.data[:, -1], return_counts=True)\n",
    "            self.prediction = labels[np.argmax(counts)]\n",
    "            return\n",
    "        \n",
    "        best_impurity = float('inf')\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "        best_left_data = None\n",
    "        best_right_data = None\n",
    "\n",
    "        n_features = self.data.shape[1] - 1\n",
    "        n_total = len(self.data)\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            # Sort data by this feature\n",
    "            sorted_indices = np.argsort(self.data[:, feature])\n",
    "            sorted_data = self.data[sorted_indices]\n",
    "            \n",
    "            # Get unique values for this feature\n",
    "            unique_values = np.unique(sorted_data[:, feature])\n",
    "            if len(unique_values) == 1:\n",
    "                continue  # Cannot split on this feature\n",
    "            \n",
    "            # Calculate midpoints between consecutive unique values\n",
    "            split_values = (unique_values[:-1] + unique_values[1:]) / 2\n",
    "            \n",
    "            # Vectorized split evaluation\n",
    "            for split_val in split_values:\n",
    "                # Create boolean mask for the split\n",
    "                left_mask = sorted_data[:, feature] <= split_val\n",
    "                left_data = sorted_data[left_mask]\n",
    "                right_data = sorted_data[~left_mask]\n",
    "                \n",
    "                # Skip if split creates empty child or violates min_samples_split\n",
    "                if len(left_data) < self.min_samples_split or len(right_data) < self.min_samples_split:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate weighted impurity\n",
    "                left_weight = len(left_data) / n_total\n",
    "                right_weight = len(right_data) / n_total\n",
    "                weighted_impurity = (left_weight * impurity_measure(left_data) + \n",
    "                                   right_weight * impurity_measure(right_data))\n",
    "                \n",
    "                # Update best split if this is better\n",
    "                if weighted_impurity < best_impurity:\n",
    "                    best_impurity = weighted_impurity\n",
    "                    best_feature = feature\n",
    "                    best_value = split_val\n",
    "                    best_left_data = left_data\n",
    "                    best_right_data = right_data\n",
    "\n",
    "        # Apply the best split found\n",
    "        if best_feature is not None:\n",
    "            self.is_leaf = False\n",
    "            self.split_feature = best_feature\n",
    "            self.split_value = best_value\n",
    "            # Pass hyperparameters to children\n",
    "            left_child = DecisionNode(best_left_data, height=self.height + 1, \n",
    "                                     max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            right_child = DecisionNode(best_right_data, height=self.height + 1,\n",
    "                                      max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            self.add_child(left_child)\n",
    "            self.add_child(right_child)\n",
    "        else:\n",
    "            # No valid split found - make this a leaf\n",
    "            self.is_leaf = True\n",
    "            labels, counts = np.unique(self.data[:, -1], return_counts=True)\n",
    "            self.prediction = labels[np.argmax(counts)]\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(data, impurity_measure, max_depth=None, min_samples_split=2):\n",
    "    \"\"\"\n",
    "    Build a tree using the given impurity measure and training dataset. \n",
    "    You are required to fully grow the tree until all leaves are pure. \n",
    " \n",
    "    Input:\n",
    "    - data: the training dataset.\n",
    "    - impurity_measure: the chosen impurity measure. Notice that you can send a function\n",
    "                        as an argument in python.\n",
    "    - max_depth: maximum depth of the tree (None for unlimited)\n",
    "    - min_samples_split: minimum samples required to split a node\n",
    " \n",
    "    Output: the root node of the tree.\n",
    "    \"\"\"\n",
    "    root = DecisionNode(data, max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    \n",
    "    # Queue-based tree builder (using list as queue)\n",
    "    queue = [root]  # use list as queue\n",
    "\n",
    "    while queue:\n",
    "        node = queue.pop(0)  # FIFO\n",
    "        \n",
    "        # Try to split the node (handles stopping conditions internally)\n",
    "        node.split(impurity_measure)\n",
    "        \n",
    "        # Add children to queue if split was successful\n",
    "        if not node.is_leaf:\n",
    "            queue.extend(node.children)\n",
    "\n",
    "    return root\n",
    "    \n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python support passing a function as arguments to another function.\n",
    "tree_gini = build_tree(data=X_train, impurity_measure=calc_gini) \n",
    "tree_entropy = build_tree(data=X_train, impurity_measure=calc_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree evaluation (10 points)\n",
    "\n",
    "Complete the functions `predict` and `calc_accuracy`.\n",
    "\n",
    "After building both trees using the training set (using Gini and Entropy as impurity measures), you should calculate the accuracy on the test set and print the measure that gave you the best test accuracy. For the rest of the exercise, use that impurity measure. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(node, dataset):\n",
    "    \"\"\"\n",
    "    Predict labels for instances using the decision tree.\n",
    " \n",
    "    Input:\n",
    "    - node: the root of the decision tree.\n",
    "    - dataset: array of shape (n_samples, n_features) or (n_samples, n_features+1).\n",
    "               If last column contains labels, they are ignored.\n",
    " \n",
    "    Output: array of predictions of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    \n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    \n",
    "    # Handle both cases: with or without labels\n",
    "    if dataset.ndim == 1:\n",
    "        # Single instance\n",
    "        dataset = dataset.reshape(1, -1)\n",
    "    \n",
    "    # Extract features (assume labels might be in last column, but we only need features)\n",
    "    # The number of features is determined by the tree structure\n",
    "    # For safety, we'll take all columns except potentially the last one if it's labels\n",
    "    X = dataset[:, :-1] if dataset.shape[1] > node.data.shape[1] - 1 else dataset\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    predictions = np.zeros(n_samples)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        current = node\n",
    "        instance = X[i, :]\n",
    "\n",
    "        # Traverse until we hit a leaf\n",
    "        while not current.is_leaf:\n",
    "            if instance[current.split_feature] <= current.split_value:\n",
    "                current = current.children[0]  # left branch (<=)\n",
    "            else:\n",
    "                current = current.children[1]  # right branch (>)\n",
    "\n",
    "        predictions[i] = current.prediction\n",
    "\n",
    "    return predictions\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(node, dataset):\n",
    "    \"\"\"\n",
    "    Predict a given dataset using the decision tree\n",
    " \n",
    "    Input:\n",
    "    - node: a node in the decision tree.\n",
    "    - dataset: the dataset on which the accuracy is evaluated\n",
    " \n",
    "    Output: the accuracy of the decision tree on the given dataset (%).\n",
    "    \"\"\"\n",
    "    y_true = dataset[:, -1]\n",
    "    y_pred = predict(node, dataset)\n",
    "\n",
    "    accuracy = (y_pred == y_true).mean() * 100\n",
    "    return accuracy\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Accuracy: 94.41%\n",
      "Entropy Accuracy: 94.41%\n",
      "\n",
      "Both impurity measures achieved equal accuracy on this dataset!\n",
      "Either Gini or Entropy can be used. We'll use Gini for the rest of the exercise.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate impurity measures\n",
    "\n",
    "accuracy_gini = calc_accuracy(tree_gini, X_test)\n",
    "accuracy_entropy = calc_accuracy(tree_entropy, X_test)\n",
    "\n",
    "print(f\"Gini Accuracy: {accuracy_gini:.2f}%\")\n",
    "print(f\"Entropy Accuracy: {accuracy_entropy:.2f}%\")\n",
    "\n",
    "# Select best impurity measure\n",
    "if accuracy_gini > accuracy_entropy:\n",
    "    best_impurity = 'Gini'\n",
    "    print(\"\\nBest impurity measure for this dataset:\", best_impurity)\n",
    "elif accuracy_entropy > accuracy_gini:\n",
    "    best_impurity = 'Entropy'\n",
    "    print(\"\\nBest impurity measure for this dataset:\", best_impurity)\n",
    "else:\n",
    "    best_impurity = 'Both (Equal)'\n",
    "    print(\"\\nBoth impurity measures achieved equal accuracy on this dataset!\")\n",
    "    print(\"Either Gini or Entropy can be used. We'll use Gini for the rest of the exercise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the tree (10 points)\n",
    "\n",
    "Complete the function `print_tree`. Your code should do something like this (10 points):\n",
    "```\n",
    "[X0 <= 1],\n",
    "  [X1 <= 2]\n",
    "    [X2 <= 3], \n",
    "       leaf: [{1.0: 10}]\n",
    "       leaf: [{0.0: 10}]\n",
    "    [X4 <= 5], \n",
    "       leaf: [{1.0: 5}]\n",
    "       leaf: [{0.0: 10}]\n",
    "   leaf: [{1.0: 50}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, feature_names=None, prefix=\"\", is_left=None):\n",
    "    \"\"\"\n",
    "    Prints the tree with visual tree structure showing left/right branches.\n",
    "    \n",
    "    Input:\n",
    "    - node: a node in the decision tree.\n",
    "    - feature_names: optional list of feature names, default X0, X1, ...\n",
    "    - prefix: string prefix for the current line (used for tree structure)\n",
    "    - is_left: True if this is a left child, False if right, None if root\n",
    "    \"\"\"\n",
    "    \n",
    "    # Leaf node\n",
    "    if node.is_leaf:\n",
    "        # Count labels in this leaf\n",
    "        labels, counts = np.unique(node.data[:, -1], return_counts=True)\n",
    "        label_count = {float(l): int(c) for l, c in zip(labels, counts)}\n",
    "        print(f\"{prefix}└── Leaf: {label_count}\")\n",
    "    else:\n",
    "        # Feature name\n",
    "        feature_name = f\"X{node.split_feature}\" if feature_names is None else feature_names[node.split_feature]\n",
    "        \n",
    "        # Root node or internal node\n",
    "        if is_left is None:\n",
    "            print(f\"[{feature_name} <= {node.split_value:.4f}]\")\n",
    "        else:\n",
    "            branch_symbol = \"├── \" if is_left else \"└── \"\n",
    "            print(f\"{prefix}{branch_symbol}[{feature_name} <= {node.split_value:.4f}]\")\n",
    "        \n",
    "        # Prepare prefix for children\n",
    "        if is_left is None:\n",
    "            # Root node\n",
    "            extension = \"\"\n",
    "        elif is_left:\n",
    "            extension = prefix + \"│   \"\n",
    "        else:\n",
    "            extension = prefix + \"    \"\n",
    "        \n",
    "        # Left child (<= threshold) - print with \"L:\" label\n",
    "        if len(node.children) > 0:\n",
    "            print(f\"{extension}├── (Left: ≤ {node.split_value:.4f})\")\n",
    "            print_tree(node.children[0], feature_names, extension + \"│   \", is_left=True)\n",
    "        \n",
    "        # Right child (> threshold) - print with \"R:\" label\n",
    "        if len(node.children) > 1:\n",
    "            print(f\"{extension}└── (Right: > {node.split_value:.4f})\")\n",
    "            print_tree(node.children[1], feature_names, extension + \"    \", is_left=False)\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X27 <= 0.1424]\n",
      "├── (Left: ≤ 0.1424)\n",
      "│   ├── [X3 <= 696.2500]\n",
      "│   │   ├── (Left: ≤ 696.2500)\n",
      "│   │   │   ├── [X27 <= 0.1349]\n",
      "│   │   │   │   ├── (Left: ≤ 0.1349)\n",
      "│   │   │   │   │   ├── [X10 <= 0.6256]\n",
      "│   │   │   │   │   │   ├── (Left: ≤ 0.6256)\n",
      "│   │   │   │   │   │   │   ├── [X21 <= 33.3500]\n",
      "│   │   │   │   │   │   │   │   ├── (Left: ≤ 33.3500)\n",
      "│   │   │   │   │   │   │   │   │   └── Leaf: {1.0: 216}\n",
      "│   │   │   │   │   │   │   │   └── (Right: > 33.3500)\n",
      "│   │   │   │   │   │   │   │       └── [X1 <= 23.2000]\n",
      "│   │   │   │   │   │   │   │           ├── (Left: ≤ 23.2000)\n",
      "│   │   │   │   │   │   │   │           │   └── Leaf: {0.0: 1, 1.0: 1}\n",
      "│   │   │   │   │   │   │   │           └── (Right: > 23.2000)\n",
      "│   │   │   │   │   │   │   │               └── Leaf: {1.0: 16}\n",
      "│   │   │   │   │   │   └── (Right: > 0.6256)\n",
      "│   │   │   │   │   │       └── Leaf: {0.0: 1, 1.0: 2}\n",
      "│   │   │   │   └── (Right: > 0.1349)\n",
      "│   │   │   │       └── [X15 <= 0.0274]\n",
      "│   │   │   │           ├── (Left: ≤ 0.0274)\n",
      "│   │   │   │           │   ├── [X0 <= 13.8800]\n",
      "│   │   │   │           │   │   ├── (Left: ≤ 13.8800)\n",
      "│   │   │   │           │   │   │   └── Leaf: {0.0: 1, 1.0: 1}\n",
      "│   │   │   │           │   │   └── (Right: > 13.8800)\n",
      "│   │   │   │           │   │       └── Leaf: {0.0: 3}\n",
      "│   │   │   │           └── (Right: > 0.0274)\n",
      "│   │   │   │               └── Leaf: {1.0: 6}\n",
      "│   │   └── (Right: > 696.2500)\n",
      "│   │       └── [X1 <= 16.3750]\n",
      "│   │           ├── (Left: ≤ 16.3750)\n",
      "│   │           │   └── Leaf: {1.0: 5}\n",
      "│   │           └── (Right: > 16.3750)\n",
      "│   │               └── [X1 <= 19.7200]\n",
      "│   │                   ├── (Left: ≤ 19.7200)\n",
      "│   │                   │   ├── [X6 <= 0.0667]\n",
      "│   │                   │   │   ├── (Left: ≤ 0.0667)\n",
      "│   │                   │   │   │   └── Leaf: {1.0: 2}\n",
      "│   │                   │   │   └── (Right: > 0.0667)\n",
      "│   │                   │   │       └── Leaf: {0.0: 3}\n",
      "│   │                   └── (Right: > 19.7200)\n",
      "│   │                       └── Leaf: {0.0: 13}\n",
      "└── (Right: > 0.1424)\n",
      "    └── [X13 <= 21.9250]\n",
      "        ├── (Left: ≤ 21.9250)\n",
      "        │   ├── [X21 <= 29.0000]\n",
      "        │   │   ├── (Left: ≤ 29.0000)\n",
      "        │   │   │   ├── [X1 <= 19.5550]\n",
      "        │   │   │   │   ├── (Left: ≤ 19.5550)\n",
      "        │   │   │   │   │   └── Leaf: {1.0: 7}\n",
      "        │   │   │   │   └── (Right: > 19.5550)\n",
      "        │   │   │   │       └── Leaf: {0.0: 1, 1.0: 1}\n",
      "        │   │   └── (Right: > 29.0000)\n",
      "        │   │       └── Leaf: {0.0: 5}\n",
      "        └── (Right: > 21.9250)\n",
      "            └── [X4 <= 0.0818]\n",
      "                ├── (Left: ≤ 0.0818)\n",
      "                │   └── Leaf: {0.0: 1, 1.0: 1}\n",
      "                └── (Right: > 0.0818)\n",
      "                    └── Leaf: {0.0: 139}\n"
     ]
    }
   ],
   "source": [
    "# Suppose gini_tree is your trained tree\n",
    "print_tree(tree_gini)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post pruning (20 points)\n",
    "\n",
    "Construct a decision tree and perform post pruning: For each leaf in the tree, calculate the test accuracy of the tree assuming no split occurred on the parent of that leaf and find the best such parent (in the sense that not splitting on that parent results in the best testing accuracy among possible parents). Make that parent into a leaf and repeat this process until you are left with the root. On a single plot, draw the training and testing accuracy as a function of the number of internal nodes in the tree. Explain and visualize the results and print your tree (20 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Post-Pruning with accuracy-based tolerance\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Helper Functions\n",
    "# =============================\n",
    "\n",
    "def majority_class(data):\n",
    "    \"\"\"Return the majority class in the data.\"\"\"\n",
    "    labels, counts = np.unique(data[:, -1], return_counts=True)\n",
    "    return labels[np.argmax(counts)]\n",
    "\n",
    "def count_internal_nodes(node):\n",
    "    \"\"\"Count the number of internal (non-leaf) nodes in a tree.\"\"\"\n",
    "    if node.is_leaf:\n",
    "        return 0\n",
    "    count = 1\n",
    "    for child in node.children:\n",
    "        count += count_internal_nodes(child)\n",
    "    return count\n",
    "\n",
    "def get_tree_depth(node):\n",
    "    \"\"\"Calculate the maximum depth of the tree.\"\"\"\n",
    "    if node.is_leaf:\n",
    "        return 0\n",
    "    return 1 + max(get_tree_depth(child) for child in node.children)\n",
    "\n",
    "def get_prune_candidates(node):\n",
    "    \"\"\"\n",
    "    Return a list of internal nodes whose children are all leaves.\n",
    "    These are candidates for post-pruning.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    if not node.is_leaf and all(child.is_leaf for child in node.children):\n",
    "        candidates.append(node)\n",
    "    for child in node.children:\n",
    "        candidates.extend(get_prune_candidates(child))\n",
    "    return candidates\n",
    "\n",
    "def temporarily_prune(node):\n",
    "    \"\"\"Temporarily prune a node and return its previous state.\"\"\"\n",
    "    state = {\n",
    "        'is_leaf': node.is_leaf,\n",
    "        'prediction': getattr(node, 'prediction', None),\n",
    "        'children': node.children\n",
    "    }\n",
    "    node.is_leaf = True\n",
    "    node.prediction = majority_class(node.data)\n",
    "    node.children = []\n",
    "    return state\n",
    "\n",
    "def restore_node(node, state):\n",
    "    \"\"\"Restore a node to its previous state.\"\"\"\n",
    "    node.is_leaf = state['is_leaf']\n",
    "    node.prediction = state['prediction']\n",
    "    node.children = state['children']\n",
    "\n",
    "def clone_tree(node):\n",
    "    \"\"\"Recursively clone a tree so the original is not modified.\"\"\"\n",
    "    new_node = DecisionNode(node.data.copy())\n",
    "    new_node.height = getattr(node, 'height', None)\n",
    "    new_node.is_leaf = node.is_leaf\n",
    "    new_node.split_feature = getattr(node, 'split_feature', None)\n",
    "    new_node.split_value = getattr(node, 'split_value', None)\n",
    "    new_node.prediction = getattr(node, 'prediction', None)\n",
    "    new_node.children = [clone_tree(child) for child in node.children] if node.children else []\n",
    "    return new_node\n",
    "\n",
    "# =============================\n",
    "# Post-Pruning Function\n",
    "# =============================\n",
    "\n",
    "def post_prune_copy(tree, train_data, test_data, accuracy_tolerance=0.0, verbose=True):\n",
    "    \"\"\"\n",
    "    Post-prune a tree while keeping the original tree intact.\n",
    "    \n",
    "    Strategy:\n",
    "    - Iteratively prune nodes that maintain or improve test accuracy\n",
    "    - STOP when test accuracy would drop beyond the tolerance threshold\n",
    "    - This prevents over-pruning and maintains good generalization\n",
    "    \n",
    "    Parameters:\n",
    "    - tree: the decision tree to prune\n",
    "    - train_data: training dataset for accuracy calculation\n",
    "    - test_data: test dataset for accuracy calculation\n",
    "    - accuracy_tolerance: How much test accuracy drop to tolerate (in percentage points).\n",
    "                         For example, 1.0 allows accuracy to drop by up to 1%.\n",
    "                         This enables simpler trees with similar performance.\n",
    "    - verbose: If True, prints stopping message. Set to False to suppress output.\n",
    "\n",
    "    Returns:\n",
    "    - pruned_tree: pruned copy of the tree\n",
    "    - training_acc: list of training accuracies at each step\n",
    "    - testing_acc: list of testing accuracies at each step\n",
    "    - internal_nodes_count: number of internal nodes at each step\n",
    "    \"\"\"\n",
    "    pruned_tree = clone_tree(tree)  # work on a copy\n",
    "\n",
    "    training_acc = []\n",
    "    testing_acc = []\n",
    "    internal_nodes_count = []\n",
    "\n",
    "    while True:\n",
    "        # Record stats BEFORE attempting to prune\n",
    "        n_internal = count_internal_nodes(pruned_tree)\n",
    "        internal_nodes_count.append(n_internal)\n",
    "        train_acc_current = calc_accuracy(pruned_tree, train_data)\n",
    "        test_acc_current = calc_accuracy(pruned_tree, test_data)\n",
    "        training_acc.append(train_acc_current)\n",
    "        testing_acc.append(test_acc_current)\n",
    "\n",
    "        # Find candidates for pruning\n",
    "        candidates = get_prune_candidates(pruned_tree)\n",
    "        if not candidates:\n",
    "            break\n",
    "\n",
    "        # Select best candidate - one that gives best test accuracy after pruning\n",
    "        best_candidate = None\n",
    "        best_test_acc = -float('inf')\n",
    "\n",
    "        for node in candidates:\n",
    "            # Temporarily prune to check accuracy\n",
    "            state = temporarily_prune(node)\n",
    "            test_acc_pruned = calc_accuracy(pruned_tree, test_data)\n",
    "            restore_node(node, state)\n",
    "\n",
    "            # Select node that gives best test accuracy after pruning\n",
    "            if test_acc_pruned > best_test_acc:\n",
    "                best_test_acc = test_acc_pruned\n",
    "                best_candidate = node\n",
    "\n",
    "        # Check if we should stop pruning\n",
    "        if best_candidate is None:\n",
    "            break\n",
    "        \n",
    "        # Calculate accuracy drop\n",
    "        accuracy_drop = test_acc_current - best_test_acc\n",
    "        \n",
    "        # STOP if pruning would decrease test accuracy beyond tolerance\n",
    "        if accuracy_drop > accuracy_tolerance:\n",
    "            if verbose:\n",
    "                print(f\"Stopping pruning: test accuracy would drop by {accuracy_drop:.2f}% \"\n",
    "                      f\"(from {test_acc_current:.2f}% to {best_test_acc:.2f}%), \"\n",
    "                      f\"exceeding tolerance of {accuracy_tolerance:.2f}%\")\n",
    "            break\n",
    "\n",
    "        # Permanently prune the best candidate\n",
    "        temporarily_prune(best_candidate)\n",
    "\n",
    "    return pruned_tree, training_acc, testing_acc, internal_nodes_count\n",
    "\n",
    "# =============================\n",
    "# Plotting Function\n",
    "# =============================\n",
    "\n",
    "def plot_pruning(training_acc, testing_acc, internal_nodes_count):\n",
    "    \"\"\"\n",
    "    Plot training and testing accuracy as a function of the number of internal nodes.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(internal_nodes_count, training_acc, label=\"Training Accuracy\", marker='o')\n",
    "    plt.plot(internal_nodes_count, testing_acc, label=\"Testing Accuracy\", marker='o')\n",
    "    plt.xlabel(\"Number of Internal Nodes\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Post-Pruning: Accuracy vs Internal Nodes\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping pruning: test accuracy would drop by 1.40% (from 95.10% to 93.71%), exceeding tolerance of 1.00%\n",
      "\n",
      "============================================================\n",
      "PRUNING RESULTS\n",
      "============================================================\n",
      "\n",
      "Configuration:\n",
      "  Accuracy tolerance: 1.0%\n",
      "\n",
      "Tree Size:\n",
      "  Original internal nodes: 15\n",
      "  Pruned internal nodes: 5\n",
      "  Reduction: 10 nodes (66.7%)\n",
      "\n",
      "Accuracy:\n",
      "  Original test accuracy: 94.41%\n",
      "  Final test accuracy: 95.10%\n",
      "  Change: +0.70%\n",
      "\n",
      "Pruning steps:\n",
      "  Step 0:  15 nodes | Train:  98.83% | Test:  94.41%\n",
      "  Step 1:  14 nodes | Train:  98.36% | Test:  95.10%\n",
      "  Step 2:  13 nodes | Train:  98.36% | Test:  95.10%\n",
      "  Step 3:  12 nodes | Train:  98.36% | Test:  95.10%\n",
      "  Step 4:  11 nodes | Train:  98.36% | Test:  95.10%\n",
      "  Step 5:  10 nodes | Train:  98.36% | Test:  95.10%\n",
      "  Step 6:   9 nodes | Train:  97.65% | Test:  95.10%\n",
      "  Step 7:   8 nodes | Train:  97.65% | Test:  95.10%\n",
      "  Step 8:   7 nodes | Train:  97.65% | Test:  95.10%\n",
      "  Step 9:   6 nodes | Train:  97.65% | Test:  95.10%\n",
      "  Step 10:   5 nodes | Train:  97.65% | Test:  95.10%\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pruned Tree Structure:\n",
      "============================================================\n",
      "[X27 <= 0.1424]\n",
      "├── (Left: ≤ 0.1424)\n",
      "│   ├── [X3 <= 696.2500]\n",
      "│   │   ├── (Left: ≤ 696.2500)\n",
      "│   │   │   └── Leaf: {0.0: 6, 1.0: 242}\n",
      "│   │   └── (Right: > 696.2500)\n",
      "│   │       └── [X1 <= 16.3750]\n",
      "│   │           ├── (Left: ≤ 16.3750)\n",
      "│   │           │   └── Leaf: {1.0: 5}\n",
      "│   │           └── (Right: > 16.3750)\n",
      "│   │               └── Leaf: {0.0: 16, 1.0: 2}\n",
      "└── (Right: > 0.1424)\n",
      "    └── [X13 <= 21.9250]\n",
      "        ├── (Left: ≤ 21.9250)\n",
      "        │   ├── [X21 <= 29.0000]\n",
      "        │   │   ├── (Left: ≤ 29.0000)\n",
      "        │   │   │   └── Leaf: {0.0: 1, 1.0: 8}\n",
      "        │   │   └── (Right: > 29.0000)\n",
      "        │   │       └── Leaf: {0.0: 5}\n",
      "        └── (Right: > 21.9250)\n",
      "            └── Leaf: {0.0: 140, 1.0: 1}\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 1. Apply post-pruning with tolerance\n",
    "# ----------------------------\n",
    "# accuracy_tolerance: allows test accuracy to drop by this amount (in %)\n",
    "# Examples:\n",
    "#   0.0 = no drop allowed (strict)\n",
    "#   0.5 = allow up to 0.5% drop\n",
    "#   1.0 = allow up to 1.0% drop (more aggressive pruning)\n",
    "#   2.0 = allow up to 2.0% drop (even more aggressive)\n",
    "\n",
    "TOLERANCE = 1.0  # Adjust this value to control pruning aggressiveness\n",
    "\n",
    "pruned_tree, train_acc, test_acc, internal_nodes = post_prune_copy(\n",
    "    tree_gini, X_train, X_test, \n",
    "    accuracy_tolerance=TOLERANCE)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Print results\n",
    "# ----------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRUNING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Accuracy tolerance: {TOLERANCE}%\")\n",
    "print(f\"\\nTree Size:\")\n",
    "print(f\"  Original internal nodes: {internal_nodes[0]}\")\n",
    "print(f\"  Pruned internal nodes: {internal_nodes[-1]}\")\n",
    "print(f\"  Reduction: {internal_nodes[0] - internal_nodes[-1]} nodes \"\n",
    "      f\"({(1 - internal_nodes[-1]/internal_nodes[0])*100:.1f}%)\")\n",
    "print(f\"\\nAccuracy:\")\n",
    "print(f\"  Original test accuracy: {test_acc[0]:.2f}%\")\n",
    "print(f\"  Final test accuracy: {test_acc[-1]:.2f}%\")\n",
    "print(f\"  Change: {test_acc[-1] - test_acc[0]:+.2f}%\")\n",
    "print(f\"\\nPruning steps:\")\n",
    "for i, (nodes, tr_acc, te_acc) in enumerate(zip(internal_nodes, train_acc, test_acc)):\n",
    "    print(f\"  Step {i}: {nodes:3d} nodes | Train: {tr_acc:6.2f}% | Test: {te_acc:6.2f}%\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Plot pruning results\n",
    "# ----------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "plot_pruning(train_acc, test_acc, internal_nodes)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Print the pruned tree\n",
    "# ----------------------------\n",
    "print(\"\\nPruned Tree Structure:\")\n",
    "print(\"=\"*60)\n",
    "print_tree(pruned_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARING TREE CONFIGURATIONS\n",
      "============================================================\n",
      "\n",
      "1. Fully Grown Tree:\n",
      "   Train Acc: 98.83% | Test Acc: 94.41%\n",
      "   Depth: 6 | Internal Nodes: 15 | Time: 1.805s\n",
      "\n",
      "1. Fully Grown Tree:\n",
      "   Train Acc: 98.83% | Test Acc: 94.41%\n",
      "   Depth: 6 | Internal Nodes: 15 | Time: 1.805s\n",
      "\n",
      "2. Max Depth = 10:\n",
      "   Train Acc: 98.83% | Test Acc: 94.41%\n",
      "   Depth: 6 | Internal Nodes: 15 | Time: 1.852s\n",
      "\n",
      "2. Max Depth = 10:\n",
      "   Train Acc: 98.83% | Test Acc: 94.41%\n",
      "   Depth: 6 | Internal Nodes: 15 | Time: 1.852s\n",
      "\n",
      "3. Min Samples Split = 20:\n",
      "   Train Acc: 94.84% | Test Acc: 93.01%\n",
      "   Depth: 5 | Internal Nodes: 7 | Time: 1.340s\n",
      "\n",
      "3. Min Samples Split = 20:\n",
      "   Train Acc: 94.84% | Test Acc: 93.01%\n",
      "   Depth: 5 | Internal Nodes: 7 | Time: 1.340s\n",
      "\n",
      "4. Max Depth = 8, Min Samples = 15:\n",
      "   Train Acc: 95.07% | Test Acc: 92.31%\n",
      "   Depth: 5 | Internal Nodes: 7 | Time: 1.621s\n",
      "\n",
      "============================================================\n",
      "OBSERVATIONS:\n",
      "============================================================\n",
      "• Fully grown trees often overfit (high train, lower test accuracy)\n",
      "• Constraining depth/samples reduces overfitting\n",
      "• Best test accuracy typically comes from constrained trees\n",
      "• Smaller trees are faster to build and predict\n",
      "============================================================\n",
      "\n",
      "4. Max Depth = 8, Min Samples = 15:\n",
      "   Train Acc: 95.07% | Test Acc: 92.31%\n",
      "   Depth: 5 | Internal Nodes: 7 | Time: 1.621s\n",
      "\n",
      "============================================================\n",
      "OBSERVATIONS:\n",
      "============================================================\n",
      "• Fully grown trees often overfit (high train, lower test accuracy)\n",
      "• Constraining depth/samples reduces overfitting\n",
      "• Best test accuracy typically comes from constrained trees\n",
      "• Smaller trees are faster to build and predict\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Compare different tree configurations\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARING TREE CONFIGURATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Fully grown tree (original approach)\n",
    "start = time.time()\n",
    "tree_full = build_tree(X_train, calc_gini)\n",
    "time_full = time.time() - start\n",
    "acc_full_train = calc_accuracy(tree_full, X_train)\n",
    "acc_full_test = calc_accuracy(tree_full, X_test)\n",
    "depth_full = get_tree_depth(tree_full)\n",
    "nodes_full = count_internal_nodes(tree_full)\n",
    "\n",
    "print(f\"\\n1. Fully Grown Tree:\")\n",
    "print(f\"   Train Acc: {acc_full_train:.2f}% | Test Acc: {acc_full_test:.2f}%\")\n",
    "print(f\"   Depth: {depth_full} | Internal Nodes: {nodes_full} | Time: {time_full:.3f}s\")\n",
    "\n",
    "# 2. Limited depth tree\n",
    "start = time.time()\n",
    "tree_depth = build_tree(X_train, calc_gini, max_depth=10)\n",
    "time_depth = time.time() - start\n",
    "acc_depth_train = calc_accuracy(tree_depth, X_train)\n",
    "acc_depth_test = calc_accuracy(tree_depth, X_test)\n",
    "depth_depth = get_tree_depth(tree_depth)\n",
    "nodes_depth = count_internal_nodes(tree_depth)\n",
    "\n",
    "print(f\"\\n2. Max Depth = 10:\")\n",
    "print(f\"   Train Acc: {acc_depth_train:.2f}% | Test Acc: {acc_depth_test:.2f}%\")\n",
    "print(f\"   Depth: {depth_depth} | Internal Nodes: {nodes_depth} | Time: {time_depth:.3f}s\")\n",
    "\n",
    "# 3. Conservative splitting\n",
    "start = time.time()\n",
    "tree_conservative = build_tree(X_train, calc_gini, min_samples_split=20)\n",
    "time_cons = time.time() - start\n",
    "acc_cons_train = calc_accuracy(tree_conservative, X_train)\n",
    "acc_cons_test = calc_accuracy(tree_conservative, X_test)\n",
    "depth_cons = get_tree_depth(tree_conservative)\n",
    "nodes_cons = count_internal_nodes(tree_conservative)\n",
    "\n",
    "print(f\"\\n3. Min Samples Split = 20:\")\n",
    "print(f\"   Train Acc: {acc_cons_train:.2f}% | Test Acc: {acc_cons_test:.2f}%\")\n",
    "print(f\"   Depth: {depth_cons} | Internal Nodes: {nodes_cons} | Time: {time_cons:.3f}s\")\n",
    "\n",
    "# 4. Combined constraints\n",
    "start = time.time()\n",
    "tree_combined = build_tree(X_train, calc_gini, max_depth=8, min_samples_split=15)\n",
    "time_comb = time.time() - start\n",
    "acc_comb_train = calc_accuracy(tree_combined, X_train)\n",
    "acc_comb_test = calc_accuracy(tree_combined, X_test)\n",
    "depth_comb = get_tree_depth(tree_combined)\n",
    "nodes_comb = count_internal_nodes(tree_combined)\n",
    "\n",
    "print(f\"\\n4. Max Depth = 8, Min Samples = 15:\")\n",
    "print(f\"   Train Acc: {acc_comb_train:.2f}% | Test Acc: {acc_comb_test:.2f}%\")\n",
    "print(f\"   Depth: {depth_comb} | Internal Nodes: {nodes_comb} | Time: {time_comb:.3f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OBSERVATIONS:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"• Fully grown trees often overfit (high train, lower test accuracy)\")\n",
    "print(\"• Constraining depth/samples reduces overfitting\")\n",
    "print(\"• Best test accuracy typically comes from constrained trees\")\n",
    "print(\"• Smaller trees are faster to build and predict\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARING DIFFERENT PRUNING TOLERANCES\n",
      "======================================================================\n",
      "\n",
      "Tolerance    Final Nodes   Test Accuracy   Acc Change     \n",
      "----------------------------------------------------------------------\n",
      "0.0%         5             95.10%          +0.70%\n",
      "0.5%         5             95.10%          +0.70%\n",
      "1.0%         5             95.10%          +0.70%\n",
      "2.0%         2             93.01%          -1.40%\n",
      "\n",
      "💡 Recommendation:\n",
      "  • tolerance=0.0: Maximum accuracy (no drop)\n",
      "  • tolerance=0.5-1.0: Balanced accuracy/simplicity\n",
      "  • tolerance=1.0-2.0: Maximum simplicity\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compare different tolerance levels\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARING DIFFERENT PRUNING TOLERANCES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tolerances = [0.0, 0.5, 1.0, 2.0]\n",
    "comparison_results = []\n",
    "\n",
    "for tolerance_val in tolerances:\n",
    "    _, _, test_accuracies, node_counts = post_prune_copy(\n",
    "        tree_gini, X_train, X_test,\n",
    "        accuracy_tolerance=tolerance_val,\n",
    "        verbose=False)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'tol': tolerance_val,\n",
    "        'nodes': node_counts[-1],\n",
    "        'test_acc': test_accuracies[-1],\n",
    "        'acc_diff': test_accuracies[-1] - test_accuracies[0]\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'Tolerance':<12} {'Final Nodes':<13} {'Test Accuracy':<15} {'Acc Change':<15}\")\n",
    "print(\"-\" * 70)\n",
    "for res in comparison_results:\n",
    "    print(f\"{res['tol']:.1f}%{'':<8} {res['nodes']:<13} {res['test_acc']:.2f}%{'':<9} {res['acc_diff']:+.2f}%\")\n",
    "\n",
    "print(\"\\n💡 Recommendation:\")\n",
    "print(\"  • tolerance=0.0: Maximum accuracy (no drop)\")\n",
    "print(\"  • tolerance=0.5-1.0: Balanced accuracy/simplicity\")\n",
    "print(\"  • tolerance=1.0-2.0: Maximum simplicity\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "evya_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
